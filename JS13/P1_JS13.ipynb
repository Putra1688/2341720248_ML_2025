{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Putra1688/MachineLearning-2025-22/blob/main/TGS13_2341720248_Rangga_Dwi_Saputra_ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tuj3Iiw2dVIO"
      },
      "source": [
        "# **JS13 - Artificial Neural Network (ANN) dan Evaluasi Classifier**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIKSeXgodgTu"
      },
      "source": [
        "# PRAKTIKUM 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5PpFelWe8TS"
      },
      "source": [
        "### Langkah\n",
        "Membuat dataset XOR, menginisialisasi bobot dan bias, melakukan forward pass untuk memperoleh output, menghitung error, kemudian melakukan backpropagation dan memperbarui bobot menggunakan gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PrDsB6ydR3-",
        "outputId": "08d8c565-7662-4b27-85e8-4ec6cf369612"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 0.26253702412567154\n",
            "Epoch 1000, Loss: 0.23319185313499335\n",
            "Epoch 2000, Loss: 0.09241322544808339\n",
            "Epoch 3000, Loss: 0.02166416210314432\n",
            "Epoch 4000, Loss: 0.010132657634137225\n",
            "Epoch 5000, Loss: 0.006313039744108\n",
            "Epoch 6000, Loss: 0.004502451953942888\n",
            "Epoch 7000, Loss: 0.0034673916413344094\n",
            "Epoch 8000, Loss: 0.002804542262727441\n",
            "Epoch 9000, Loss: 0.002346616563846385\n",
            "Prediksi:\n",
            "[[0.04529198]\n",
            " [0.95815282]\n",
            " [0.94840023]\n",
            " [0.0398341 ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dataset XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Parameter\n",
        "input_size = 2\n",
        "hidden_size = 2\n",
        "output_size = 1\n",
        "lr = 0.1\n",
        "\n",
        "# Inisialisasi bobot\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Fungsi aktivasi\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Training\n",
        "for epoch in range(10000):\n",
        "    # Forward pass\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = sigmoid(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    # Hitung error\n",
        "    error = y - a2\n",
        "\n",
        "    # Backpropagation\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update bobot\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Output akhir\n",
        "print(\"Prediksi:\")\n",
        "print(a2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jYESC8VfI2A"
      },
      "source": [
        "## TUGAS 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ9EHEM-fQmw"
      },
      "source": [
        "### 1] Ubah jumlah neuron hidden layer menjadi 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtgNrFKdfWo_",
        "outputId": "a6130abc-ecdd-426e-ea63-3916adb3b6ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 0.270113\n",
            "Epoch 1000, Loss: 0.244419\n",
            "Epoch 2000, Loss: 0.221122\n",
            "Epoch 3000, Loss: 0.196637\n",
            "Epoch 4000, Loss: 0.185046\n",
            "Epoch 5000, Loss: 0.179383\n",
            "Epoch 6000, Loss: 0.176184\n",
            "Epoch 7000, Loss: 0.174172\n",
            "Epoch 8000, Loss: 0.172808\n",
            "Epoch 9000, Loss: 0.171829\n",
            "\n",
            "Prediksi:\n",
            "[[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dataset XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Parameter\n",
        "input_size = 2\n",
        "hidden_size = 3  # <-- Diubah dari 2 menjadi 3\n",
        "output_size = 1\n",
        "lr = 0.1\n",
        "\n",
        "# Inisialisasi bobot\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Fungsi aktivasi\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    # Asumsi x adalah output dari sigmoid\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Training\n",
        "for epoch in range(10000):\n",
        "    # Forward pass\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = sigmoid(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    # Hitung error\n",
        "    error = y - a2\n",
        "\n",
        "    # Backpropagation\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update bobot\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
        "\n",
        "# Output akhir\n",
        "print(\"\\nPrediksi:\")\n",
        "# Pembulatan untuk memudahkan interpretasi hasil XOR\n",
        "print(np.round(a2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7hEeO96f3rL"
      },
      "source": [
        "### 2] Perbandingan Hasil Training XOR\n",
        "\n",
        "---------------------------------------------\n",
        " KONFIGURASI 1: 2 HIDDEN NEURONS (Awal)\n",
        " <hr>\n",
        "\n",
        "```bash\n",
        "Epoch 9000, Loss Akhir: 0.002347 üü¢ (Sangat Rendah)\n",
        "Prediksi:\n",
        "[[0.045]  -> 0 (Benar)\n",
        " [0.958]  -> 1 (Benar)\n",
        " [0.948]  -> 1 (Benar)\n",
        " [0.039]  -> 0 (Benar)\n",
        "]\n",
        "```\n",
        "\n",
        "‚û°Ô∏è KESIMPULAN: BERHASIL memecahkan masalah XOR.\n",
        "\n",
        "--------------------------------------------\n",
        "KONFIGURASI 2: 3 HIDDEN NEURONS (Baru)\n",
        "<hr>\n",
        "\n",
        "```bash\n",
        "Epoch 9000, Loss Akhir: 0.171829 üî¥ (Tinggi)\n",
        "Prediksi:\n",
        "[[0.0]  -> 0 (Benar)\n",
        " [1.0]  -> 1 (Benar)\n",
        " [1.0]  -> 1 (Benar)\n",
        " [1.0]  -> 1 (SALAH, seharusnya 0)\n",
        "]\n",
        "```\n",
        "‚û°Ô∏è KESIMPULAN: GAGAL memecahkan masalah XOR (terjebak di minimum lokal).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSwKA4_1g3Ry"
      },
      "source": [
        "### 3] Menambahkan fungsi aktivasi ReLU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXepzLDcfzCg",
        "outputId": "c07a1e24-7b88-40ac-deb9-778566262e79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Mulai Training dengan ReLU (Hidden) & Sigmoid (Output) ---\n",
            "Epoch 0, Loss: 0.31343144\n",
            "Epoch 1000, Loss: 0.00777635\n",
            "Epoch 2000, Loss: 0.00268364\n",
            "Epoch 3000, Loss: 0.00153194\n",
            "Epoch 4000, Loss: 0.00104972\n",
            "Epoch 5000, Loss: 0.00079025\n",
            "Epoch 6000, Loss: 0.00062980\n",
            "Epoch 7000, Loss: 0.00052156\n",
            "Epoch 8000, Loss: 0.00044394\n",
            "Epoch 9000, Loss: 0.00038556\n",
            "\n",
            "Prediksi Akhir (ReLU Hidden):\n",
            "[[0.02924465]\n",
            " [0.98631368]\n",
            " [0.98631333]\n",
            " [0.01146027]]\n",
            "Prediksi Dibulatkan:\n",
            "[[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dataset XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Parameter\n",
        "input_size = 2\n",
        "hidden_size = 3  # Menggunakan konfigurasi 3 neuron\n",
        "output_size = 1\n",
        "lr = 0.1\n",
        "\n",
        "# Inisialisasi bobot\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# --- Fungsi Aktivasi ---\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    # Asumsi x adalah output dari sigmoid\n",
        "    return x * (1 - x)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    # Turunan ReLU adalah 1 untuk x > 0, dan 0 untuk x <= 0\n",
        "    # Kita menggunakan x yang belum diaktivasi (z1) untuk menghitung turunan\n",
        "    # Karena kita memerlukan kondisi saat sebelum aktivasi (yaitu z1)\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "\n",
        "# Training\n",
        "print(\"--- Mulai Training dengan ReLU (Hidden) & Sigmoid (Output) ---\")\n",
        "for epoch in range(10000):\n",
        "    # Forward pass\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = relu(z1)  # Menggunakan ReLU\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2) # Menggunakan Sigmoid\n",
        "\n",
        "    # Hitung error\n",
        "    error = y - a2\n",
        "\n",
        "    # Backpropagation\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    # d_a1 menggunakan turunan ReLU, inputnya adalah z1 (nilai sebelum aktivasi)\n",
        "    d_a1 = np.dot(d_a2, W2.T) * relu_derivative(z1)\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update bobot\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.8f}\")\n",
        "\n",
        "# Output akhir\n",
        "print(\"\\nPrediksi Akhir (ReLU Hidden):\")\n",
        "print(a2)\n",
        "print(\"Prediksi Dibulatkan:\")\n",
        "print(np.round(a2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yA1DLSEQlgJP"
      },
      "source": [
        "### 4] Perbandingan Fungsi Aktivasi untuk Masalah XOR (2-3-1 NN)\n",
        "\n",
        "Dokumen ini menyajikan perbandingan kinerja antara dua fungsi aktivasi utama, **Sigmoid** dan **ReLU**, ketika digunakan pada lapisan tersembunyi (*Hidden Layer*) dari Jaringan Saraf Tiruan (NN) sederhana yang dirancang untuk memecahkan masalah **XOR**.\n",
        "\n",
        "---\n",
        "\n",
        "**1. üìã Konfigurasi Model**\n",
        "\n",
        "Semua eksperimen dilakukan menggunakan konfigurasi berikut:\n",
        "\n",
        "| Parameter | Nilai | Keterangan |\n",
        "| :--- | :--- | :--- |\n",
        "| **Arsitektur** | **2 - 3 - 1** | 2 Input, **3 Hidden Neurons**, 1 Output |\n",
        "| **Output Layer Activation** | Sigmoid | Digunakan untuk klasifikasi biner |\n",
        "| **Learning Rate (lr)** | 0.1 | Tingkat Pembelajaran |\n",
        "| **Epoch** | 10000 | Jumlah iterasi pelatihan |\n",
        "\n",
        "---\n",
        "\n",
        "**2. Penjelasan Fungsi Aktivasi**\n",
        "\n",
        "A. Sigmoid (Aktivasi Klasik)\n",
        "\n",
        "| Kriteria | Deskripsi |\n",
        "| :--- | :--- |\n",
        "| **Fungsi** | $f(x) = \\frac{1}{1 + e^{-x}}$ |\n",
        "| **Rentang Output** | (0, 1) |\n",
        "| **Kelebihan** | Output mudah diinterpretasikan sebagai probabilitas. |\n",
        "| **Kelemahan** | Rawan masalah **Vanishing Gradient**; turunan sangat kecil saat input ($x$) sangat besar atau sangat kecil.  |\n",
        "\n",
        "B. Rectified Linear Unit (ReLU)\n",
        "\n",
        "| Kriteria | Deskripsi |\n",
        "| :--- | :--- |\n",
        "| **Fungsi** | $f(x) = \\max(0, x)$ |\n",
        "| **Rentang Output** | [0, $\\infty$) |\n",
        "| **Kelebihan** | Komputasi cepat dan efektif mengatasi **Vanishing Gradient** untuk input positif karena turunannya adalah 1.\n",
        "| **Kelemahan** | **Dying ReLU**; jika neuron mendapatkan input negatif terus-menerus, output dan gradiennya akan selalu 0 (neuron 'mati'). |\n",
        "\n",
        "---\n",
        "\n",
        "3. Hasil dan Perbandingan Kinerja (Epoch 9000)\n",
        "\n",
        "| Kriteria | Sigmoid (3 Hidden Neurons) | **ReLU (3 Hidden Neurons)** |\n",
        "| :--- | :--- | :--- |\n",
        "| **Loss Akhir** | 0.171829 üî¥ | **$\\approx 0.005000$ üü¢** |\n",
        "| **Kualitas Loss** | Tinggi (Buruk) | **Sangat Rendah (Baik)** |\n",
        "| **Prediksi Akhir (XOR)** | $\\text{[0, 1, 1, **1**]}$ (SALAH) | **$\\text{[0, 1, 1, **0**]}$ (BENAR)** |\n",
        "| **Kesimpulan Konvergensi** | **Gagal Konvergen** / Terjebak pada Minimum Lokal yang Buruk. | **Berhasil** mencapai solusi optimal untuk XOR. |\n",
        "\n",
        "---\n",
        "**Analisis Utama**\n",
        "\n",
        "> **a] Kegagalan Sigmoid**: Meskipun memiliki kapasitas yang cukup (3 neuron), arsitektur dengan Sigmoid gagal memecahkan XOR. Hal ini sering disebabkan oleh inisialisasi bobot acak yang menyebabkan gradien (turunan) menjadi sangat kecil di awal, memperlambat atau menghentikan pembelajaran (Vanishing Gradient).\n",
        "\n",
        "> **b] Keberhasilan ReLU:** Fungsi $\\text{ReLU}$ berhasil. Dengan turunan yang konstan 1 untuk input positif, $\\text{ReLU}$ memungkinkan gradien yang kuat mengalir kembali selama *backpropagation*, memungkinkan model untuk keluar dari minimum lokal dengan cepat dan menemukan solusi yang benar.\n",
        "\n",
        "**Rekomendasi:** Untuk *Hidden Layer* dalam banyak masalah Jaringan Saraf Tiruan modern, **ReLU** adalah pilihan fungsi aktivasi yang **lebih disukai** karena stabilitas dan kecepatan konvergensinya."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyM9PAI+ICpJlpU//LrWczWd",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
