{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Putra1688/MachineLearning-2025-22/blob/main/TG6_2341720248_Rangga_Dwi_Saputra_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or5kPoh1Fi3q"
      },
      "source": [
        "# **JS06 - ANN (Approximate Nearest Neighbors)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYwoac30G9vB"
      },
      "source": [
        "# **PRAKTIKUM 6**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwgKhOsiJqV1",
        "outputId": "dd06a3df-a7aa-4fb4-d0dc-7036036d2298"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1289902402.py:10: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
            "  df = kagglehub.load_dataset(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/bwandowando/spotify-songs-with-attributes-and-lyrics?dataset_version_number=19&file_name=songs_with_attributes_and_lyrics.csv...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.44G/1.44G [00:44<00:00, 35.1MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 5 records:                        id             name  \\\n",
            "0  0Prct5TDjAnEgIqbxcldY9                !   \n",
            "1  2ASl4wirkeYm3OWZxXKYuq               !!   \n",
            "2  69lcggVPmOr9cvPx9kLiiN  !!! - Interlude   \n",
            "3  4U7dlZjg1s9pjdppqZy0fm   !!De Repente!!   \n",
            "4  4v1IBp3Y3rpkWmWzIlkYju   !!De Repente!!   \n",
            "\n",
            "                               album_name       artists  danceability  energy  \\\n",
            "0                              UNDEN!ABLE  ['HELLYEAH']         0.415  0.6050   \n",
            "1                                     NaN       Yxngxr1         0.788  0.6480   \n",
            "2                       Where I Belong EP    ['Glowie']         0.000  0.0354   \n",
            "3  Un Palo Al Agua (20 Grandes Canciones)   ['Rosendo']         0.657  0.8820   \n",
            "4                          Fuera De Lugar   ['Rosendo']         0.659  0.8930   \n",
            "\n",
            "  key  loudness mode  speechiness  acousticness  instrumentalness  liveness  \\\n",
            "0   7   -11.157    1       0.0575       0.00116          0.838000    0.4710   \n",
            "1   7    -9.135    0       0.3150       0.90000          0.000000    0.1760   \n",
            "2   7   -20.151    0       0.0000       0.90800          0.000000    0.4790   \n",
            "3   5    -6.340    1       0.0385       0.00740          0.000013    0.0474   \n",
            "4   5    -8.531    1       0.0411       0.09220          0.000019    0.0534   \n",
            "\n",
            "   valence    tempo  duration_ms  \\\n",
            "0    0.193  100.059      79500.0   \n",
            "1    0.287   79.998     114000.0   \n",
            "2    0.000    0.000      11413.0   \n",
            "3    0.939  123.588     198173.0   \n",
            "4    0.951  123.600     199827.0   \n",
            "\n",
            "                                              lyrics  \n",
            "0  He said he came from Jamaica,\\n he owned a cou...  \n",
            "1  Fucked a bitch, now she running with my kids\\n...  \n",
            "2                     Oh, my God, I'm going crazy\\n   \n",
            "3  Continuamente se extraña la gente si no puede ...  \n",
            "4  Continuamente se extraña la gente si no puede ...  \n"
          ]
        }
      ],
      "source": [
        "# Install dependencies as needed:\n",
        "# pip install kagglehub[pandas-datasets]\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "# Set the path to the file you'd like to load\n",
        "file_path = \"songs_with_attributes_and_lyrics.csv\"\n",
        "\n",
        "# Load the latest version\n",
        "df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"bwandowando/spotify-songs-with-attributes-and-lyrics\",\n",
        "  file_path,\n",
        "  pandas_kwargs={\"nrows\": 100000}\n",
        ")\n",
        "\n",
        "print(\"First 5 records:\", df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bra8jGytHAr5",
        "outputId": "62e5c237-1d2f-4ce7-fac7-e08c890fdc5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memuat 100000 baris data dari Kaggle...\n",
            "Using Colab cache for faster access to the 'spotify-songs-with-attributes-and-lyrics' dataset.\n",
            "Database Size: 100000 vektor, Query Size: 1000 vektor.\n",
            "--- Memulai Uji Waktu ---\n",
            "Exact NN done in 0.834 s\n",
            "Annoy done in 2.013 s\n",
            "HNSW done in 13.648 s\n",
            "FAISS IVF done in 0.140 s\n",
            "\n",
            "=== Ringkasan Waktu (detik) ===\n",
            "Exact NN : 0.834\n",
            "Annoy    : 2.013\n",
            "HNSW     : 13.648\n",
            "FAISS    : 0.140\n",
            "\n",
            "=== Perbandingan Hasil (Top-5 Neighbors untuk Query Pertama) ===\n",
            "Exact NN: [    0 61511 85956  3836 35205]\n",
            "Annoy:    [0, 61511, 3836, 35205, 41311]\n",
            "HNSW:     [    0 61511 85956  3836 35205]\n",
            "FAISS:    [    0 61511 85956  3836 35205]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import faiss\n",
        "from annoy import AnnoyIndex\n",
        "import hnswlib\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from kagglehub import dataset_load, KaggleDatasetAdapter\n",
        "\n",
        "# Menggunakan 100,000 baris sebagai sampel untuk mengurangi waktu loading\n",
        "N_SAMPLES = 100000\n",
        "N_QUERIES = 1000 # HANYA MENGUJI 1000 QUERY UNTUK KECEPATAN\n",
        "\n",
        "print(f\"Memuat {N_SAMPLES} baris data dari Kaggle...\")\n",
        "\n",
        "try:\n",
        "    # Menggunakan fungsi dataset_load() yang disarankan\n",
        "    df = dataset_load(\n",
        "      KaggleDatasetAdapter.PANDAS,\n",
        "      \"bwandowando/spotify-songs-with-attributes-and-lyrics\",\n",
        "      \"songs_with_attributes_and_lyrics.csv\",\n",
        "      pandas_kwargs={\"nrows\": N_SAMPLES}\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"Error saat memuat data: {e}\")\n",
        "    print(\"\\n⚠️ Menggunakan data acak (dummy) untuk melanjutkan pengujian algoritma.\")\n",
        "    # Fallback ke data acak jika gagal memuat data Kaggle\n",
        "    np.random.seed(42)\n",
        "    D = 9 # Jumlah dimensi (fitur)\n",
        "    df = pd.DataFrame(np.random.random((N_SAMPLES, D)),\n",
        "                      columns=['f1','f2','f3','f4','f5','f6','f7','f8','f9'])\n",
        "\n",
        "\n",
        "features = ['danceability', 'energy', 'loudness', 'speechiness',\n",
        "            'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo']\n",
        "\n",
        "# Memastikan hanya fitur yang tersedia yang digunakan jika menggunakan data Kaggle\n",
        "if len(df.columns) > 9: # Jika data dari Kaggle berhasil dimuat (memiliki fitur lengkap)\n",
        "    X = df[features].values\n",
        "else: # Jika menggunakan data dummy\n",
        "    X = df.values\n",
        "\n",
        "\n",
        "# Standarisasi fitur\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X).astype('float32') # FAISS butuh float32\n",
        "\n",
        "# Tentukan set data yang digunakan:\n",
        "X_database = X_scaled             # Seluruh 100k sampel digunakan sebagai database untuk diindeks\n",
        "X_query = X_scaled[:N_QUERIES]    # Hanya 1000 sampel pertama digunakan untuk query\n",
        "k = 10                            # Jumlah nearest neighbors\n",
        "\n",
        "\n",
        "print(f\"Database Size: {X_database.shape[0]} vektor, Query Size: {X_query.shape[0]} vektor.\")\n",
        "print(\"--- Memulai Uji Waktu ---\")\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 1. Exact NN (brute-force) - Baseline Akurasi 100%\n",
        "# ----------------------------------------------------\n",
        "start = time.time()\n",
        "nn = NearestNeighbors(n_neighbors=k, algorithm='brute', metric='euclidean')\n",
        "nn.fit(X_database)\n",
        "# Query hanya X_query\n",
        "dist_exact, idx_exact = nn.kneighbors(X_query)\n",
        "time_exact = time.time() - start\n",
        "print(f\"Exact NN done in {time_exact:.3f} s\")\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 2. Annoy - Perlu looping (paling lambat dari ANN)\n",
        "# ----------------------------------------------------\n",
        "start = time.time()\n",
        "f = X_database.shape[1]\n",
        "index_annoy = AnnoyIndex(f, 'euclidean')\n",
        "for i, v in enumerate(X_database):\n",
        "    index_annoy.add_item(i, v)\n",
        "index_annoy.build(10) # 10 trees\n",
        "# Query hanya X_query (menggunakan list comprehension untuk loop yang efisien)\n",
        "idx_annoy = [index_annoy.get_nns_by_vector(v, k) for v in X_query]\n",
        "time_annoy = time.time() - start\n",
        "print(f\"Annoy done in {time_annoy:.3f} s\")\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 3. HNSW - Sangat Cepat (Symmetric Search)\n",
        "# ----------------------------------------------------\n",
        "start = time.time()\n",
        "D = X_database.shape[1]\n",
        "p_hnsw = hnswlib.Index(space='l2', dim=D)\n",
        "p_hnsw.init_index(max_elements=X_database.shape[0], ef_construction=200, M=16)\n",
        "p_hnsw.add_items(X_database)\n",
        "p_hnsw.set_ef(200) # Efektif untuk pencarian yang cepat dan akurat\n",
        "# Query X_query\n",
        "idx_hnsw, _ = p_hnsw.knn_query(X_query, k=k)\n",
        "time_hnsw = time.time() - start\n",
        "print(f\"HNSW done in {time_hnsw:.3f} s\")\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 4. FAISS IVF (Inverted File Index) - Cepat & Skalabel\n",
        "# ----------------------------------------------------\n",
        "start = time.time()\n",
        "D = X_database.shape[1] # Menggunakan D dari sesi sebelumnya\n",
        "quantizer = faiss.IndexFlatL2(D)\n",
        "# KOREKSI: Hapus 'nlist=' dan 'metric='\n",
        "# Formatnya adalah: IndexIVFFlat(quantizer, dim, nlist, metric)\n",
        "index_faiss = faiss.IndexIVFFlat(quantizer, D, 100, faiss.METRIC_L2)\n",
        "index_faiss.train(X_database) # Pastikan menggunakan X_database (100k sampel)\n",
        "index_faiss.add(X_database)\n",
        "index_faiss.nprobe = 10\n",
        "dist_faiss, idx_faiss = index_faiss.search(X_query, k) # Query X_query (1k sampel)\n",
        "time_faiss = time.time() - start\n",
        "print(f\"FAISS IVF done in {time_faiss:.3f} s\")\n",
        "\n",
        "# -------------------------------\n",
        "# Tampilkan ringkasan waktu & hasil\n",
        "# -------------------------------\n",
        "print(\"\\n=== Ringkasan Waktu (detik) ===\")\n",
        "print(f\"Exact NN : {time_exact:.3f}\")\n",
        "print(f\"Annoy    : {time_annoy:.3f}\")\n",
        "print(f\"HNSW     : {time_hnsw:.3f}\")\n",
        "print(f\"FAISS    : {time_faiss:.3f}\")\n",
        "\n",
        "print(\"\\n=== Perbandingan Hasil (Top-5 Neighbors untuk Query Pertama) ===\")\n",
        "# idx_exact[0] adalah daftar indeks tetangga terdekat yang akurat (Exact NN) untuk item pertama\n",
        "print(f\"Exact NN: {idx_exact[0][:5]}\")\n",
        "print(f\"Annoy:    {idx_annoy[0][:5]}\")\n",
        "print(f\"HNSW:     {idx_hnsw[0][:5]}\")\n",
        "print(f\"FAISS:    {idx_faiss[0][:5]}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPJ1WW6pPcwO/AQ01U1ACVW",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
